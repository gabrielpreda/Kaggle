{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b692202d",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "papermill": {
     "duration": 0.00787,
     "end_time": "2025-03-29T20:27:39.706345",
     "exception": false,
     "start_time": "2025-03-29T20:27:39.698475",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "\n",
    "<center><img src=\"https://www.geeky-gadgets.com/wp-content/uploads/2025/03/google-gemma-3-advanced-ai-models.webp\"></img></center>\n",
    "\n",
    "# Introduction\n",
    "This Notebook will explore how to prompt Gemma 3, using KerasNLP. \n",
    "\n",
    "\n",
    "## What is Gemma?\n",
    "Gemma is a family of lightweight, state-of-the art open models built from the same research and technology used to create the Gemini models. At the 3rd generation now, Gemma 3 comes in 4 sizes, **1B**, **4B**, **12B** and **27B**, both pretrained and instruction finetuned versions.   \n",
    "\n",
    "Models **4B**, **12B**, **27B** brings an extended context window (up to **128K**) as well as **multi-modality** (text and image). \n",
    "\n",
    "The **1B** model, although incredibly compact, is not only very fast but is also quite powerful.\n",
    "\n",
    "Will use the **1B** model, on CPU only, without any accelerator.\n",
    "\n",
    "\n",
    "# Prerequisites\n",
    "\n",
    "## Install packages\n",
    "\n",
    "We will install Keras and KerasNLP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8a6b554f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-29T20:27:39.722467Z",
     "iopub.status.busy": "2025-03-29T20:27:39.722078Z",
     "iopub.status.idle": "2025-03-29T20:27:55.630102Z",
     "shell.execute_reply": "2025-03-29T20:27:55.628564Z"
    },
    "papermill": {
     "duration": 15.918807,
     "end_time": "2025-03-29T20:27:55.632704",
     "exception": false,
     "start_time": "2025-03-29T20:27:39.713897",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.3/731.3 kB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m25.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\n",
      "tensorflow-decision-forests 1.10.0 requires tensorflow==2.17.0, but you have tensorflow 2.17.1 which is incompatible.\u001b[0m\u001b[31m\r\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install -q -U keras-nlp\n",
    "!pip install -q -U keras"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e99328c1",
   "metadata": {
    "papermill": {
     "duration": 0.007855,
     "end_time": "2025-03-29T20:27:55.649050",
     "exception": false,
     "start_time": "2025-03-29T20:27:55.641195",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "56168016",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-29T20:27:55.667590Z",
     "iopub.status.busy": "2025-03-29T20:27:55.667241Z",
     "iopub.status.idle": "2025-03-29T20:28:15.044187Z",
     "shell.execute_reply": "2025-03-29T20:28:15.043005Z"
    },
    "papermill": {
     "duration": 19.388027,
     "end_time": "2025-03-29T20:28:15.046365",
     "exception": false,
     "start_time": "2025-03-29T20:27:55.658338",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "import keras_nlp\n",
    "from keras_nlp.samplers import TopKSampler\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26c4e80b",
   "metadata": {
    "papermill": {
     "duration": 0.007917,
     "end_time": "2025-03-29T20:28:15.062236",
     "exception": false,
     "start_time": "2025-03-29T20:28:15.054319",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Select the backend. Keras is a high-level, multi-framework deep learning API designed for simplicity and ease of use. Keras 3 lets you choose the backend: TensorFlow, JAX, or PyTorch. For this Notebook, we will choose jax as backend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eeed1336",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-29T20:28:15.079107Z",
     "iopub.status.busy": "2025-03-29T20:28:15.078376Z",
     "iopub.status.idle": "2025-03-29T20:28:15.083414Z",
     "shell.execute_reply": "2025-03-29T20:28:15.082265Z"
    },
    "papermill": {
     "duration": 0.01529,
     "end_time": "2025-03-29T20:28:15.085213",
     "exception": false,
     "start_time": "2025-03-29T20:28:15.069923",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"KERAS_BACKEND\"] = \"jax\"\n",
    "os.environ[\"XLA_PYTHON_CLIENT_MEM_FRACTION\"] = \"0.9\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a81081aa",
   "metadata": {
    "papermill": {
     "duration": 0.007236,
     "end_time": "2025-03-29T20:28:15.100189",
     "exception": false,
     "start_time": "2025-03-29T20:28:15.092953",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Initialize the model\n",
    "\n",
    "Make sure to use `Gemma3CausalLM` to initialize the model, not the `GemmaCausalLM`. Otherwise the Gemma3 backbone will not be recognized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "13aa22aa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-29T20:28:15.116726Z",
     "iopub.status.busy": "2025-03-29T20:28:15.116316Z",
     "iopub.status.idle": "2025-03-29T20:28:48.732070Z",
     "shell.execute_reply": "2025-03-29T20:28:48.730769Z"
    },
    "papermill": {
     "duration": 33.626491,
     "end_time": "2025-03-29T20:28:48.734355",
     "exception": false,
     "start_time": "2025-03-29T20:28:15.107864",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "gemma_lm = keras_nlp.models.Gemma3CausalLM.from_preset(\"/kaggle/input/gemma3/keras/gemma3_1b/1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ca36d904",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-29T20:28:48.753264Z",
     "iopub.status.busy": "2025-03-29T20:28:48.752796Z",
     "iopub.status.idle": "2025-03-29T20:28:48.962134Z",
     "shell.execute_reply": "2025-03-29T20:28:48.960779Z"
    },
    "papermill": {
     "duration": 0.221668,
     "end_time": "2025-03-29T20:28:48.964697",
     "exception": false,
     "start_time": "2025-03-29T20:28:48.743029",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer = keras_nlp.models.Gemma3Tokenizer.from_preset(\"/kaggle/input/gemma3/keras/gemma3_1b/1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ff30bb0",
   "metadata": {
    "papermill": {
     "duration": 0.007559,
     "end_time": "2025-03-29T20:28:48.980122",
     "exception": false,
     "start_time": "2025-03-29T20:28:48.972563",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Let's verify the model now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b0925d6c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-29T20:28:48.997580Z",
     "iopub.status.busy": "2025-03-29T20:28:48.997154Z",
     "iopub.status.idle": "2025-03-29T20:28:49.035358Z",
     "shell.execute_reply": "2025-03-29T20:28:49.034256Z"
    },
    "papermill": {
     "duration": 0.0493,
     "end_time": "2025-03-29T20:28:49.037311",
     "exception": false,
     "start_time": "2025-03-29T20:28:48.988011",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Preprocessor: \"gemma3_causal_lm_preprocessor\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mPreprocessor: \"gemma3_causal_lm_preprocessor\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                                                  </span>┃<span style=\"font-weight: bold\">                                   Config </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ gemma3_tokenizer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Gemma3Tokenizer</span>)                            │                      Vocab size: <span style=\"color: #00af00; text-decoration-color: #00af00\">262,144</span> │\n",
       "└───────────────────────────────────────────────────────────────┴──────────────────────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                                                 \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m                                  Config\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ gemma3_tokenizer (\u001b[38;5;33mGemma3Tokenizer\u001b[0m)                            │                      Vocab size: \u001b[38;5;34m262,144\u001b[0m │\n",
       "└───────────────────────────────────────────────────────────────┴──────────────────────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"gemma3_causal_lm\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"gemma3_causal_lm\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                  </span>┃<span style=\"font-weight: bold\"> Output Shape              </span>┃<span style=\"font-weight: bold\">         Param # </span>┃<span style=\"font-weight: bold\"> Connected to               </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ padding_mask (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)              │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                          │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ token_ids (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)              │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                          │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ gemma3_backbone               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1152</span>)        │     <span style=\"color: #00af00; text-decoration-color: #00af00\">999,885,952</span> │ padding_mask[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],        │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Gemma3Backbone</span>)              │                           │                 │ token_ids[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ token_embedding               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">262144</span>)      │     <span style=\"color: #00af00; text-decoration-color: #00af00\">301,989,888</span> │ gemma3_backbone[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ReversibleEmbedding</span>)         │                           │                 │                            │\n",
       "└───────────────────────────────┴───────────────────────────┴─────────────────┴────────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                 \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to              \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ padding_mask (\u001b[38;5;33mInputLayer\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)              │               \u001b[38;5;34m0\u001b[0m │ -                          │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ token_ids (\u001b[38;5;33mInputLayer\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)              │               \u001b[38;5;34m0\u001b[0m │ -                          │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ gemma3_backbone               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1152\u001b[0m)        │     \u001b[38;5;34m999,885,952\u001b[0m │ padding_mask[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],        │\n",
       "│ (\u001b[38;5;33mGemma3Backbone\u001b[0m)              │                           │                 │ token_ids[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ token_embedding               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m262144\u001b[0m)      │     \u001b[38;5;34m301,989,888\u001b[0m │ gemma3_backbone[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n",
       "│ (\u001b[38;5;33mReversibleEmbedding\u001b[0m)         │                           │                 │                            │\n",
       "└───────────────────────────────┴───────────────────────────┴─────────────────┴────────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">999,885,952</span> (3.72 GB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m999,885,952\u001b[0m (3.72 GB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">999,885,952</span> (3.72 GB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m999,885,952\u001b[0m (3.72 GB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "gemma_lm.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a4318b6",
   "metadata": {
    "papermill": {
     "duration": 0.007977,
     "end_time": "2025-03-29T20:28:49.053996",
     "exception": false,
     "start_time": "2025-03-29T20:28:49.046019",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Test the model\n",
    "\n",
    "Let's define a simple prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "07bd5e55",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-29T20:28:49.071837Z",
     "iopub.status.busy": "2025-03-29T20:28:49.071442Z",
     "iopub.status.idle": "2025-03-29T20:29:13.753940Z",
     "shell.execute_reply": "2025-03-29T20:29:13.752561Z"
    },
    "papermill": {
     "duration": 24.693949,
     "end_time": "2025-03-29T20:29:13.756249",
     "exception": false,
     "start_time": "2025-03-29T20:28:49.062300",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "instructions = \"You are an AI that answers in short, complete sentences only.\\n\"\n",
    "prompt = (\n",
    "    f\"{instructions}\"\n",
    "    \"Question: {question}\\n\"\n",
    "    \"Answer:\"\n",
    ")\n",
    "\n",
    "output = gemma_lm.generate(\n",
    "    prompt.format(question=\"What is the temperature of the Moon?\"),\n",
    "    max_length=40, \n",
    "    stop_token_ids=[tokenizer.token_to_id(\"\\n\")]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fd79730d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-29T20:29:13.777196Z",
     "iopub.status.busy": "2025-03-29T20:29:13.776564Z",
     "iopub.status.idle": "2025-03-29T20:29:13.782934Z",
     "shell.execute_reply": "2025-03-29T20:29:13.781437Z"
    },
    "papermill": {
     "duration": 0.019685,
     "end_time": "2025-03-29T20:29:13.785131",
     "exception": false,
     "start_time": "2025-03-29T20:29:13.765446",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What is the temperature of the Moon?\n",
      "Answer: The temperature of the Moon is 100 degrees Celsius\n"
     ]
    }
   ],
   "source": [
    "answer = output.replace(instructions, \"\").strip()\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce1b47ec",
   "metadata": {
    "papermill": {
     "duration": 0.008629,
     "end_time": "2025-03-29T20:29:13.803001",
     "exception": false,
     "start_time": "2025-03-29T20:29:13.794372",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Functions to generate and format the output\n",
    "\n",
    "We define a function to generate the answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6dc4009a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-29T20:29:13.821729Z",
     "iopub.status.busy": "2025-03-29T20:29:13.821349Z",
     "iopub.status.idle": "2025-03-29T20:29:13.827373Z",
     "shell.execute_reply": "2025-03-29T20:29:13.826212Z"
    },
    "papermill": {
     "duration": 0.017399,
     "end_time": "2025-03-29T20:29:13.829371",
     "exception": false,
     "start_time": "2025-03-29T20:29:13.811972",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate_answer(question, \n",
    "                    instructions=\"You are an AI that answers in short, complete sentences only.\\n\", \n",
    "                    max_length=40):\n",
    "    prompt = (\n",
    "        f\"{instructions}\"\n",
    "        \"Question: {question}\\n\"\n",
    "        \"Answer:\"\n",
    "    )\n",
    "    output = gemma_lm.generate(\n",
    "        prompt.format(question=question),\n",
    "        max_length=max_length, \n",
    "        stop_token_ids=[tokenizer.token_to_id(\"\\n\")]\n",
    "    )    \n",
    "    answer = output.replace(instructions, \"\").strip()\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98ad0c51",
   "metadata": {
    "papermill": {
     "duration": 0.009,
     "end_time": "2025-03-29T20:29:13.847982",
     "exception": false,
     "start_time": "2025-03-29T20:29:13.838982",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "We define a function to format the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "87751bcf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-29T20:29:13.866331Z",
     "iopub.status.busy": "2025-03-29T20:29:13.865976Z",
     "iopub.status.idle": "2025-03-29T20:29:13.871908Z",
     "shell.execute_reply": "2025-03-29T20:29:13.870467Z"
    },
    "papermill": {
     "duration": 0.017656,
     "end_time": "2025-03-29T20:29:13.874255",
     "exception": false,
     "start_time": "2025-03-29T20:29:13.856599",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from IPython.display import display, Markdown\n",
    "\n",
    "def colorize_text(text):\n",
    "    for word, color in zip([\"Reasoning\", \"Question\", \"Answer\", \"Explanation\", \"Total time\"], [\"blue\", \"red\", \"green\", \"darkblue\",  \"magenta\"]):\n",
    "        text = text.replace(f\"{word}:\", f\"\\n\\n**<font color='{color}'>{word}:</font>**\")\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c351f8e",
   "metadata": {
    "papermill": {
     "duration": 0.008125,
     "end_time": "2025-03-29T20:29:13.891218",
     "exception": false,
     "start_time": "2025-03-29T20:29:13.883093",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Now let's combine both."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9fd1493b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-29T20:29:13.910138Z",
     "iopub.status.busy": "2025-03-29T20:29:13.909680Z",
     "iopub.status.idle": "2025-03-29T20:29:13.915086Z",
     "shell.execute_reply": "2025-03-29T20:29:13.913692Z"
    },
    "papermill": {
     "duration": 0.016935,
     "end_time": "2025-03-29T20:29:13.917105",
     "exception": false,
     "start_time": "2025-03-29T20:29:13.900170",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate_format_answer(question, \n",
    "                    instructions=\"You are an AI that answers in short, complete sentences only.\\n\", \n",
    "                    max_length=40):\n",
    "    t = time()\n",
    "    answer = generate_answer(question, instructions, max_length)\n",
    "    display(Markdown(colorize_text(f\"{answer}\\n\\nTotal time: {round(time()-t, 2)} sec.\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bc6328b5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-29T20:29:13.937352Z",
     "iopub.status.busy": "2025-03-29T20:29:13.937008Z",
     "iopub.status.idle": "2025-03-29T20:29:16.610749Z",
     "shell.execute_reply": "2025-03-29T20:29:16.609642Z"
    },
    "papermill": {
     "duration": 2.685447,
     "end_time": "2025-03-29T20:29:16.612598",
     "exception": false,
     "start_time": "2025-03-29T20:29:13.927151",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "\n",
       "**<font color='red'>Question:</font>** What is the surface temperature of the Moon?\n",
       "\n",
       "\n",
       "**<font color='green'>Answer:</font>** 100 degrees Celsius\n",
       "\n",
       "\n",
       "\n",
       "**<font color='magenta'>Total time:</font>** 2.67 sec."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "generate_format_answer(\"What is the surface temperature of the Moon?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9d827e4",
   "metadata": {
    "papermill": {
     "duration": 0.008722,
     "end_time": "2025-03-29T20:29:16.630666",
     "exception": false,
     "start_time": "2025-03-29T20:29:16.621944",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Let's ask some simple common knowledge questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2d13aa26",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-29T20:29:16.651306Z",
     "iopub.status.busy": "2025-03-29T20:29:16.650906Z",
     "iopub.status.idle": "2025-03-29T20:29:20.568103Z",
     "shell.execute_reply": "2025-03-29T20:29:20.566982Z"
    },
    "papermill": {
     "duration": 3.930375,
     "end_time": "2025-03-29T20:29:20.569845",
     "exception": false,
     "start_time": "2025-03-29T20:29:16.639470",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "\n",
       "**<font color='red'>Question:</font>** When was the 30 years war?\n",
       "\n",
       "\n",
       "**<font color='green'>Answer:</font>** 1618-1648\n",
       "\n",
       "\n",
       "\n",
       "**<font color='magenta'>Total time:</font>** 3.91 sec."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "answer = generate_format_answer(\"When was the 30 years war?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3729a6ba",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-29T20:29:20.588802Z",
     "iopub.status.busy": "2025-03-29T20:29:20.588377Z",
     "iopub.status.idle": "2025-03-29T20:29:24.370337Z",
     "shell.execute_reply": "2025-03-29T20:29:24.369226Z"
    },
    "papermill": {
     "duration": 3.793526,
     "end_time": "2025-03-29T20:29:24.372304",
     "exception": false,
     "start_time": "2025-03-29T20:29:20.578778",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "\n",
       "**<font color='red'>Question:</font>** When was the Great War?\n",
       "\n",
       "\n",
       "**<font color='green'>Answer:</font>** 1914-1918\n",
       "\n",
       "\n",
       "\n",
       "**<font color='magenta'>Total time:</font>** 3.78 sec."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "answer = generate_format_answer(\"When was the Great War?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4ef9fc0f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-29T20:29:24.393759Z",
     "iopub.status.busy": "2025-03-29T20:29:24.393359Z",
     "iopub.status.idle": "2025-03-29T20:29:26.667488Z",
     "shell.execute_reply": "2025-03-29T20:29:26.666506Z"
    },
    "papermill": {
     "duration": 2.287867,
     "end_time": "2025-03-29T20:29:26.669237",
     "exception": false,
     "start_time": "2025-03-29T20:29:24.381370",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "\n",
       "**<font color='red'>Question:</font>** When was the USA-Spanish war?\n",
       "\n",
       "\n",
       "**<font color='green'>Answer:</font>** 1898\n",
       "\n",
       "\n",
       "\n",
       "**<font color='magenta'>Total time:</font>** 2.27 sec."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "generate_format_answer(\"When was the USA-Spanish war?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b0ff8375",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-29T20:29:26.690043Z",
     "iopub.status.busy": "2025-03-29T20:29:26.689616Z",
     "iopub.status.idle": "2025-03-29T20:29:28.284002Z",
     "shell.execute_reply": "2025-03-29T20:29:28.282382Z"
    },
    "papermill": {
     "duration": 1.60795,
     "end_time": "2025-03-29T20:29:28.286301",
     "exception": false,
     "start_time": "2025-03-29T20:29:26.678351",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "\n",
       "**<font color='red'>Question:</font>** Who was the president before 1st mandate of Donald Trump?\n",
       "\n",
       "\n",
       "**<font color='green'>Answer:</font>** Barack Obama\n",
       "\n",
       "\n",
       "\n",
       "**<font color='magenta'>Total time:</font>** 1.59 sec."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "generate_format_answer(\"Who was the president before 1st mandate of Donald Trump?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9dc67f39",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-29T20:29:28.307617Z",
     "iopub.status.busy": "2025-03-29T20:29:28.307178Z",
     "iopub.status.idle": "2025-03-29T20:29:31.673331Z",
     "shell.execute_reply": "2025-03-29T20:29:31.672234Z"
    },
    "papermill": {
     "duration": 3.379498,
     "end_time": "2025-03-29T20:29:31.675340",
     "exception": false,
     "start_time": "2025-03-29T20:29:28.295842",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "\n",
       "**<font color='red'>Question:</font>** When was the attack on Pearl Harbor?\n",
       "\n",
       "\n",
       "**<font color='green'>Answer:</font>** December 7, 1941\n",
       "\n",
       "\n",
       "\n",
       "**<font color='magenta'>Total time:</font>** 3.36 sec."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "generate_format_answer(\"When was the attack on Pearl Harbor?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3160a4c0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-29T20:29:31.696765Z",
     "iopub.status.busy": "2025-03-29T20:29:31.696377Z",
     "iopub.status.idle": "2025-03-29T20:29:33.807661Z",
     "shell.execute_reply": "2025-03-29T20:29:33.806554Z"
    },
    "papermill": {
     "duration": 2.124569,
     "end_time": "2025-03-29T20:29:33.809536",
     "exception": false,
     "start_time": "2025-03-29T20:29:31.684967",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "\n",
       "**<font color='red'>Question:</font>** Who was the next shogon after Yeiatsu Tokugawa?\n",
       "\n",
       "\n",
       "**<font color='green'>Answer:</font>** Tokugawa Ieyasu\n",
       "\n",
       "\n",
       "\n",
       "**<font color='magenta'>Total time:</font>** 2.11 sec."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "generate_format_answer(\"Who was the next shogon after Yeiatsu Tokugawa?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "647fd4f3",
   "metadata": {
    "papermill": {
     "duration": 0.008984,
     "end_time": "2025-03-29T20:29:33.827949",
     "exception": false,
     "start_time": "2025-03-29T20:29:33.818965",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "This answer is not correct. The correct answer is Hidetada Tokugawa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "034478b3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-29T20:29:33.847569Z",
     "iopub.status.busy": "2025-03-29T20:29:33.847224Z",
     "iopub.status.idle": "2025-03-29T20:29:35.383251Z",
     "shell.execute_reply": "2025-03-29T20:29:35.382075Z"
    },
    "papermill": {
     "duration": 1.547989,
     "end_time": "2025-03-29T20:29:35.385160",
     "exception": false,
     "start_time": "2025-03-29T20:29:33.837171",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "\n",
       "**<font color='red'>Question:</font>** Who was the first American president?\n",
       "\n",
       "\n",
       "**<font color='green'>Answer:</font>** George Washington\n",
       "\n",
       "\n",
       "\n",
       "**<font color='magenta'>Total time:</font>** 1.53 sec."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "generate_format_answer(\"Who was the first American president?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51abfcb2",
   "metadata": {
    "papermill": {
     "duration": 0.009529,
     "end_time": "2025-03-29T20:29:35.404867",
     "exception": false,
     "start_time": "2025-03-29T20:29:35.395338",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Let's explore more about technology\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "54fc8d69",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-29T20:29:35.426117Z",
     "iopub.status.busy": "2025-03-29T20:29:35.425639Z",
     "iopub.status.idle": "2025-03-29T20:30:03.341002Z",
     "shell.execute_reply": "2025-03-29T20:30:03.339188Z"
    },
    "papermill": {
     "duration": 27.928284,
     "end_time": "2025-03-29T20:30:03.343066",
     "exception": false,
     "start_time": "2025-03-29T20:29:35.414782",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "\n",
       "**<font color='red'>Question:</font>** What is XGBoost?\n",
       "\n",
       "\n",
       "**<font color='green'>Answer:</font>** XGBoost is a machine learning algorithm that is used to solve classification problems. It is a gradient boosting algorithm that is used to\n",
       "\n",
       "\n",
       "\n",
       "**<font color='magenta'>Total time:</font>** 27.91 sec."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "generate_format_answer(\"What is XGBoost?\", \n",
    "                       max_length=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f6d1ea07",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-29T20:30:03.364558Z",
     "iopub.status.busy": "2025-03-29T20:30:03.364185Z",
     "iopub.status.idle": "2025-03-29T20:30:08.468395Z",
     "shell.execute_reply": "2025-03-29T20:30:08.466877Z"
    },
    "papermill": {
     "duration": 5.117566,
     "end_time": "2025-03-29T20:30:08.470451",
     "exception": false,
     "start_time": "2025-03-29T20:30:03.352885",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "\n",
       "**<font color='red'>Question:</font>** What is AlphaFold?\n",
       "\n",
       "\n",
       "**<font color='green'>Answer:</font>** AlphaFold is a computer program that predicts the structure of proteins. It is\n",
       "\n",
       "\n",
       "\n",
       "**<font color='magenta'>Total time:</font>** 5.1 sec."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "generate_format_answer(\"What is AlphaFold?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8d9614c4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-29T20:30:08.494164Z",
     "iopub.status.busy": "2025-03-29T20:30:08.493640Z",
     "iopub.status.idle": "2025-03-29T20:30:13.574903Z",
     "shell.execute_reply": "2025-03-29T20:30:13.573740Z"
    },
    "papermill": {
     "duration": 5.095779,
     "end_time": "2025-03-29T20:30:13.577185",
     "exception": false,
     "start_time": "2025-03-29T20:30:08.481406",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "\n",
       "**<font color='red'>Question:</font>** What is RandomForest?\n",
       "\n",
       "\n",
       "**<font color='green'>Answer:</font>** A machine learning algorithm that uses a combination of decision trees to make predictions.\n",
       "\n",
       "\n",
       "\n",
       "**<font color='magenta'>Total time:</font>** 5.07 sec."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "generate_format_answer(\"What is RandomForest?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d79ff0a",
   "metadata": {
    "papermill": {
     "duration": 0.010815,
     "end_time": "2025-03-29T20:30:13.598195",
     "exception": false,
     "start_time": "2025-03-29T20:30:13.587380",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Let's do math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6e57d79c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-29T20:30:13.619469Z",
     "iopub.status.busy": "2025-03-29T20:30:13.619098Z",
     "iopub.status.idle": "2025-03-29T20:30:16.737278Z",
     "shell.execute_reply": "2025-03-29T20:30:16.736208Z"
    },
    "papermill": {
     "duration": 3.131153,
     "end_time": "2025-03-29T20:30:16.739250",
     "exception": false,
     "start_time": "2025-03-29T20:30:13.608097",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "\n",
       "**<font color='red'>Question:</font>** What is 123 + 11?\n",
       "\n",
       "\n",
       "**<font color='green'>Answer:</font>** 134\n",
       "\n",
       "You are an AI\n",
       "\n",
       "\n",
       "\n",
       "**<font color='magenta'>Total time:</font>** 3.11 sec."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "generate_format_answer(\"What is 123 + 11?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7c5f69cb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-29T20:30:16.760857Z",
     "iopub.status.busy": "2025-03-29T20:30:16.760484Z",
     "iopub.status.idle": "2025-03-29T20:30:18.778129Z",
     "shell.execute_reply": "2025-03-29T20:30:18.776770Z"
    },
    "papermill": {
     "duration": 2.030805,
     "end_time": "2025-03-29T20:30:18.780002",
     "exception": false,
     "start_time": "2025-03-29T20:30:16.749197",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "\n",
       "**<font color='red'>Question:</font>** What is 25 x 25?\n",
       "\n",
       "\n",
       "**<font color='green'>Answer:</font>** 625\n",
       "\n",
       "\n",
       "\n",
       "**<font color='magenta'>Total time:</font>** 2.01 sec."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "generate_format_answer(\"What is 25 x 25?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f93addfb",
   "metadata": {
    "papermill": {
     "duration": 0.010737,
     "end_time": "2025-03-29T20:30:18.801304",
     "exception": false,
     "start_time": "2025-03-29T20:30:18.790567",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# More math and Python code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6826761a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-29T20:30:18.823494Z",
     "iopub.status.busy": "2025-03-29T20:30:18.823147Z",
     "iopub.status.idle": "2025-03-29T20:30:18.827368Z",
     "shell.execute_reply": "2025-03-29T20:30:18.826295Z"
    },
    "papermill": {
     "duration": 0.017623,
     "end_time": "2025-03-29T20:30:18.829295",
     "exception": false,
     "start_time": "2025-03-29T20:30:18.811672",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "You are an AI assistant designed to write simple Python code.\n",
    "Please answer with the listing of the Python code.\n",
    "Question: {question}\n",
    "Answer:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "480490c0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-29T20:30:18.851677Z",
     "iopub.status.busy": "2025-03-29T20:30:18.851243Z",
     "iopub.status.idle": "2025-03-29T20:30:46.623256Z",
     "shell.execute_reply": "2025-03-29T20:30:46.622000Z"
    },
    "papermill": {
     "duration": 27.785913,
     "end_time": "2025-03-29T20:30:46.625310",
     "exception": false,
     "start_time": "2025-03-29T20:30:18.839397",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "You are an AI assistant designed to write simple Python code.\n",
       "Please answer with the listing of the Python code.\n",
       "\n",
       "\n",
       "**<font color='red'>Question:</font>** Please write a function in Python to calculate the area of a circle of radius r\n",
       "\n",
       "\n",
       "**<font color='green'>Answer:</font>**\n",
       "def area(r):\n",
       "return 3.14 * r * r\n",
       "print(area(10))\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "**<font color='magenta'>Total time:</font>** 27.77 sec."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "t = time()\n",
    "response = gemma_lm.generate(prompt.format(question=\"Please write a function in Python to calculate the area of a circle of radius r\"), max_length=128)\n",
    "display(Markdown(colorize_text(f\"{response}\\n\\nTotal time: {round(time()-t, 2)} sec.\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "afaf9fa2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-29T20:30:46.648404Z",
     "iopub.status.busy": "2025-03-29T20:30:46.647990Z",
     "iopub.status.idle": "2025-03-29T20:31:27.704761Z",
     "shell.execute_reply": "2025-03-29T20:31:27.703456Z"
    },
    "papermill": {
     "duration": 41.080393,
     "end_time": "2025-03-29T20:31:27.716602",
     "exception": false,
     "start_time": "2025-03-29T20:30:46.636209",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "You are an AI assistant designed to write simple Python code.\n",
       "Please answer with the listing of the Python code.\n",
       "\n",
       "\n",
       "**<font color='red'>Question:</font>** Please write a function in Python to order a list\n",
       "\n",
       "\n",
       "**<font color='green'>Answer:</font>**\n",
       "def order_list(list):\n",
       "for i in range(len(list)):\n",
       "for j in range(i+1, len(list)):\n",
       "if list[i] > list[j]:\n",
       "list[i], list[j] = list[j], list[i]\n",
       "return list\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "**<font color='magenta'>Total time:</font>** 41.05 sec."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "t = time()\n",
    "response = gemma_lm.generate(prompt.format(question=\"Please write a function in Python to order a list\"), max_length=256)\n",
    "display(Markdown(colorize_text(f\"{response}\\n\\nTotal time: {round(time()-t, 2)} sec.\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca8b55d6",
   "metadata": {
    "papermill": {
     "duration": 0.009811,
     "end_time": "2025-03-29T20:31:27.737254",
     "exception": false,
     "start_time": "2025-03-29T20:31:27.727443",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Conclusions\n",
    "\n",
    "\n",
    "The Gemma 3 model (1B) from Google DeepMind is powerful for its size and can answer to a lot of questions about history, politics, as well as simple math. It is not great with writing code and formating the output when asked to write code."
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "modelId": 279036,
     "modelInstanceId": 257756,
     "sourceId": 301815,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30918,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 235.325785,
   "end_time": "2025-03-29T20:31:31.391433",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-03-29T20:27:36.065648",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
