{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6db9747e",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "papermill": {
     "duration": 0.009523,
     "end_time": "2025-03-29T20:03:35.101012",
     "exception": false,
     "start_time": "2025-03-29T20:03:35.091489",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Introduction\n",
    "This Notebook will explore how to prompt Gemma 3, using KerasNLP.\n",
    "\n",
    "## What is Gemma?\n",
    "Gemma is a family of lightweight, state-of-the art open models built from the same research and technology used to create the Gemini models. At the 3rd generation now, Gemma 3 comes in 4 sizes, 1B, 4B, 12B and 27B, both pretrained and instruction finetuned versions.   \n",
    "\n",
    "Models 4B, 12B, 27B brings an extended context window (up to 128K) as well as multi-modality. \n",
    "\n",
    "The 1B model, although incredibly compact, is not only very fast but is also quite powerful.\n",
    "\n",
    "We will demonstrate here the features of 1B model while running on CPU only.\n",
    "\n",
    "\n",
    "# Prerequisites\n",
    "\n",
    "## Install packages\n",
    "\n",
    "We will install Keras and KerasNLP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8a8885c5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-29T20:03:35.117248Z",
     "iopub.status.busy": "2025-03-29T20:03:35.116757Z",
     "iopub.status.idle": "2025-03-29T20:03:49.696527Z",
     "shell.execute_reply": "2025-03-29T20:03:49.695283Z"
    },
    "papermill": {
     "duration": 14.590147,
     "end_time": "2025-03-29T20:03:49.698744",
     "exception": false,
     "start_time": "2025-03-29T20:03:35.108597",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.3/731.3 kB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m21.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\n",
      "tensorflow-decision-forests 1.10.0 requires tensorflow==2.17.0, but you have tensorflow 2.17.1 which is incompatible.\u001b[0m\u001b[31m\r\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install -q -U keras-nlp\n",
    "!pip install -q -U keras"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f92de9a",
   "metadata": {
    "papermill": {
     "duration": 0.007324,
     "end_time": "2025-03-29T20:03:49.714126",
     "exception": false,
     "start_time": "2025-03-29T20:03:49.706802",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3eb1696d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-29T20:03:49.731554Z",
     "iopub.status.busy": "2025-03-29T20:03:49.731207Z",
     "iopub.status.idle": "2025-03-29T20:04:08.995181Z",
     "shell.execute_reply": "2025-03-29T20:04:08.993819Z"
    },
    "papermill": {
     "duration": 19.274958,
     "end_time": "2025-03-29T20:04:08.997300",
     "exception": false,
     "start_time": "2025-03-29T20:03:49.722342",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "import keras_nlp\n",
    "from keras_nlp.samplers import TopKSampler\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f966246",
   "metadata": {
    "papermill": {
     "duration": 0.007485,
     "end_time": "2025-03-29T20:04:09.013799",
     "exception": false,
     "start_time": "2025-03-29T20:04:09.006314",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Select the backend. Keras is a high-level, multi-framework deep learning API designed for simplicity and ease of use. Keras 3 lets you choose the backend: TensorFlow, JAX, or PyTorch. For this Notebook, we will choose jax as backend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a23ff6f0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-29T20:04:09.030610Z",
     "iopub.status.busy": "2025-03-29T20:04:09.029934Z",
     "iopub.status.idle": "2025-03-29T20:04:09.034769Z",
     "shell.execute_reply": "2025-03-29T20:04:09.033534Z"
    },
    "papermill": {
     "duration": 0.015031,
     "end_time": "2025-03-29T20:04:09.036448",
     "exception": false,
     "start_time": "2025-03-29T20:04:09.021417",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"KERAS_BACKEND\"] = \"jax\"\n",
    "os.environ[\"XLA_PYTHON_CLIENT_MEM_FRACTION\"] = \"0.9\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d27f8a07",
   "metadata": {
    "papermill": {
     "duration": 0.007286,
     "end_time": "2025-03-29T20:04:09.051715",
     "exception": false,
     "start_time": "2025-03-29T20:04:09.044429",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Initialize the model\n",
    "\n",
    "Make sure to use `Gemma3CausalLM` to initialize the model, not the `GemmaCausalLM`. Otherwise the Gemma3 backbone will not be recognized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "668d28cb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-29T20:04:09.068127Z",
     "iopub.status.busy": "2025-03-29T20:04:09.067757Z",
     "iopub.status.idle": "2025-03-29T20:04:35.184199Z",
     "shell.execute_reply": "2025-03-29T20:04:35.183002Z"
    },
    "papermill": {
     "duration": 26.126836,
     "end_time": "2025-03-29T20:04:35.186266",
     "exception": false,
     "start_time": "2025-03-29T20:04:09.059430",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "gemma_lm = keras_nlp.models.Gemma3CausalLM.from_preset(\"/kaggle/input/gemma3/keras/gemma3_1b/1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5b8495c7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-29T20:04:35.206161Z",
     "iopub.status.busy": "2025-03-29T20:04:35.205758Z",
     "iopub.status.idle": "2025-03-29T20:04:35.443313Z",
     "shell.execute_reply": "2025-03-29T20:04:35.442160Z"
    },
    "papermill": {
     "duration": 0.248621,
     "end_time": "2025-03-29T20:04:35.445394",
     "exception": false,
     "start_time": "2025-03-29T20:04:35.196773",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer = keras_nlp.models.Gemma3Tokenizer.from_preset(\"/kaggle/input/gemma3/keras/gemma3_1b/1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fb2b119",
   "metadata": {
    "papermill": {
     "duration": 0.007257,
     "end_time": "2025-03-29T20:04:35.460702",
     "exception": false,
     "start_time": "2025-03-29T20:04:35.453445",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Let's verify the model now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "73b894f1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-29T20:04:35.477304Z",
     "iopub.status.busy": "2025-03-29T20:04:35.476956Z",
     "iopub.status.idle": "2025-03-29T20:04:35.516139Z",
     "shell.execute_reply": "2025-03-29T20:04:35.515097Z"
    },
    "papermill": {
     "duration": 0.049267,
     "end_time": "2025-03-29T20:04:35.517782",
     "exception": false,
     "start_time": "2025-03-29T20:04:35.468515",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Preprocessor: \"gemma3_causal_lm_preprocessor\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mPreprocessor: \"gemma3_causal_lm_preprocessor\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                                                  </span>┃<span style=\"font-weight: bold\">                                   Config </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ gemma3_tokenizer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Gemma3Tokenizer</span>)                            │                      Vocab size: <span style=\"color: #00af00; text-decoration-color: #00af00\">262,144</span> │\n",
       "└───────────────────────────────────────────────────────────────┴──────────────────────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                                                 \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m                                  Config\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ gemma3_tokenizer (\u001b[38;5;33mGemma3Tokenizer\u001b[0m)                            │                      Vocab size: \u001b[38;5;34m262,144\u001b[0m │\n",
       "└───────────────────────────────────────────────────────────────┴──────────────────────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"gemma3_causal_lm\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"gemma3_causal_lm\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                  </span>┃<span style=\"font-weight: bold\"> Output Shape              </span>┃<span style=\"font-weight: bold\">         Param # </span>┃<span style=\"font-weight: bold\"> Connected to               </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ padding_mask (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)              │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                          │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ token_ids (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)              │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                          │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ gemma3_backbone               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1152</span>)        │     <span style=\"color: #00af00; text-decoration-color: #00af00\">999,885,952</span> │ padding_mask[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],        │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Gemma3Backbone</span>)              │                           │                 │ token_ids[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ token_embedding               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">262144</span>)      │     <span style=\"color: #00af00; text-decoration-color: #00af00\">301,989,888</span> │ gemma3_backbone[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ReversibleEmbedding</span>)         │                           │                 │                            │\n",
       "└───────────────────────────────┴───────────────────────────┴─────────────────┴────────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                 \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to              \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ padding_mask (\u001b[38;5;33mInputLayer\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)              │               \u001b[38;5;34m0\u001b[0m │ -                          │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ token_ids (\u001b[38;5;33mInputLayer\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)              │               \u001b[38;5;34m0\u001b[0m │ -                          │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ gemma3_backbone               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1152\u001b[0m)        │     \u001b[38;5;34m999,885,952\u001b[0m │ padding_mask[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],        │\n",
       "│ (\u001b[38;5;33mGemma3Backbone\u001b[0m)              │                           │                 │ token_ids[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ token_embedding               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m262144\u001b[0m)      │     \u001b[38;5;34m301,989,888\u001b[0m │ gemma3_backbone[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n",
       "│ (\u001b[38;5;33mReversibleEmbedding\u001b[0m)         │                           │                 │                            │\n",
       "└───────────────────────────────┴───────────────────────────┴─────────────────┴────────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">999,885,952</span> (3.72 GB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m999,885,952\u001b[0m (3.72 GB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">999,885,952</span> (3.72 GB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m999,885,952\u001b[0m (3.72 GB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "gemma_lm.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6662c77",
   "metadata": {
    "papermill": {
     "duration": 0.008203,
     "end_time": "2025-03-29T20:04:35.534688",
     "exception": false,
     "start_time": "2025-03-29T20:04:35.526485",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Test the model\n",
    "\n",
    "Let's define a simple prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "380e738f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-29T20:04:35.553278Z",
     "iopub.status.busy": "2025-03-29T20:04:35.552806Z",
     "iopub.status.idle": "2025-03-29T20:04:59.550929Z",
     "shell.execute_reply": "2025-03-29T20:04:59.549843Z"
    },
    "papermill": {
     "duration": 24.009995,
     "end_time": "2025-03-29T20:04:59.553336",
     "exception": false,
     "start_time": "2025-03-29T20:04:35.543341",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "instructions = \"You are an AI that answers in short, complete sentences only.\\n\"\n",
    "prompt = (\n",
    "    f\"{instructions}\"\n",
    "    \"Question: {question}\\n\"\n",
    "    \"Answer:\"\n",
    ")\n",
    "\n",
    "output = gemma_lm.generate(\n",
    "    prompt.format(question=\"What is the temperature of the Moon?\"),\n",
    "    max_length=40, \n",
    "    stop_token_ids=[tokenizer.token_to_id(\"\\n\")]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "abe0394a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-29T20:04:59.572500Z",
     "iopub.status.busy": "2025-03-29T20:04:59.572136Z",
     "iopub.status.idle": "2025-03-29T20:04:59.577046Z",
     "shell.execute_reply": "2025-03-29T20:04:59.575974Z"
    },
    "papermill": {
     "duration": 0.016333,
     "end_time": "2025-03-29T20:04:59.578846",
     "exception": false,
     "start_time": "2025-03-29T20:04:59.562513",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What is the temperature of the Moon?\n",
      "Answer: The temperature of the Moon is 100 degrees Celsius\n"
     ]
    }
   ],
   "source": [
    "answer = output.replace(instructions, \"\").strip()\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "624da2e3",
   "metadata": {
    "papermill": {
     "duration": 0.008383,
     "end_time": "2025-03-29T20:04:59.596405",
     "exception": false,
     "start_time": "2025-03-29T20:04:59.588022",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Functions to generate and format the output\n",
    "\n",
    "We define a function to generate the answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "af5283ff",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-29T20:04:59.614905Z",
     "iopub.status.busy": "2025-03-29T20:04:59.614521Z",
     "iopub.status.idle": "2025-03-29T20:04:59.619788Z",
     "shell.execute_reply": "2025-03-29T20:04:59.618846Z"
    },
    "papermill": {
     "duration": 0.016719,
     "end_time": "2025-03-29T20:04:59.621627",
     "exception": false,
     "start_time": "2025-03-29T20:04:59.604908",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate_answer(question, \n",
    "                    instructions=\"You are an AI that answers in short, complete sentences only.\\n\", \n",
    "                    max_length=40):\n",
    "    prompt = (\n",
    "        f\"{instructions}\"\n",
    "        \"Question: {question}\\n\"\n",
    "        \"Answer:\"\n",
    "    )\n",
    "    output = gemma_lm.generate(\n",
    "        prompt.format(question=question),\n",
    "        max_length=max_length, \n",
    "        stop_token_ids=[tokenizer.token_to_id(\"\\n\")]\n",
    "    )    \n",
    "    answer = output.replace(instructions, \"\").strip()\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccac255f",
   "metadata": {
    "papermill": {
     "duration": 0.008187,
     "end_time": "2025-03-29T20:04:59.638565",
     "exception": false,
     "start_time": "2025-03-29T20:04:59.630378",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "We define a function to format the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a40ff84b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-29T20:04:59.657205Z",
     "iopub.status.busy": "2025-03-29T20:04:59.656832Z",
     "iopub.status.idle": "2025-03-29T20:04:59.662384Z",
     "shell.execute_reply": "2025-03-29T20:04:59.661105Z"
    },
    "papermill": {
     "duration": 0.017249,
     "end_time": "2025-03-29T20:04:59.664446",
     "exception": false,
     "start_time": "2025-03-29T20:04:59.647197",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from IPython.display import display, Markdown\n",
    "\n",
    "def colorize_text(text):\n",
    "    for word, color in zip([\"Reasoning\", \"Question\", \"Answer\", \"Explanation\", \"Total time\"], [\"blue\", \"red\", \"green\", \"darkblue\",  \"magenta\"]):\n",
    "        text = text.replace(f\"{word}:\", f\"\\n\\n**<font color='{color}'>{word}:</font>**\")\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "498f12d8",
   "metadata": {
    "papermill": {
     "duration": 0.008118,
     "end_time": "2025-03-29T20:04:59.681176",
     "exception": false,
     "start_time": "2025-03-29T20:04:59.673058",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Now let's combine both."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5b39e170",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-29T20:04:59.699389Z",
     "iopub.status.busy": "2025-03-29T20:04:59.699048Z",
     "iopub.status.idle": "2025-03-29T20:04:59.704413Z",
     "shell.execute_reply": "2025-03-29T20:04:59.703212Z"
    },
    "papermill": {
     "duration": 0.01675,
     "end_time": "2025-03-29T20:04:59.706361",
     "exception": false,
     "start_time": "2025-03-29T20:04:59.689611",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate_format_answer(question, \n",
    "                    instructions=\"You are an AI that answers in short, complete sentences only.\\n\", \n",
    "                    max_length=40):\n",
    "    t = time()\n",
    "    answer = generate_answer(question, instructions, max_length)\n",
    "    display(Markdown(colorize_text(f\"{answer}\\n\\nTotal time: {round(time()-t, 2)} sec.\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5ecbb705",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-29T20:04:59.724934Z",
     "iopub.status.busy": "2025-03-29T20:04:59.724544Z",
     "iopub.status.idle": "2025-03-29T20:05:02.356465Z",
     "shell.execute_reply": "2025-03-29T20:05:02.355268Z"
    },
    "papermill": {
     "duration": 2.643706,
     "end_time": "2025-03-29T20:05:02.358859",
     "exception": false,
     "start_time": "2025-03-29T20:04:59.715153",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "\n",
       "**<font color='red'>Question:</font>** What is the surface temperature of the Moon?\n",
       "\n",
       "\n",
       "**<font color='green'>Answer:</font>** 100 degrees Celsius\n",
       "\n",
       "\n",
       "\n",
       "**<font color='magenta'>Total time:</font>** 2.63 sec."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "generate_format_answer(\"What is the surface temperature of the Moon?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53e94852",
   "metadata": {
    "papermill": {
     "duration": 0.008262,
     "end_time": "2025-03-29T20:05:02.376136",
     "exception": false,
     "start_time": "2025-03-29T20:05:02.367874",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Let's ask some simple common knowledge questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5fe1b21b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-29T20:05:02.395199Z",
     "iopub.status.busy": "2025-03-29T20:05:02.394700Z",
     "iopub.status.idle": "2025-03-29T20:05:06.032370Z",
     "shell.execute_reply": "2025-03-29T20:05:06.031278Z"
    },
    "papermill": {
     "duration": 3.649573,
     "end_time": "2025-03-29T20:05:06.034400",
     "exception": false,
     "start_time": "2025-03-29T20:05:02.384827",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "\n",
       "**<font color='red'>Question:</font>** When was the 30 years war?\n",
       "\n",
       "\n",
       "**<font color='green'>Answer:</font>** 1618-1648\n",
       "\n",
       "\n",
       "\n",
       "**<font color='magenta'>Total time:</font>** 3.63 sec."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "answer = generate_format_answer(\"When was the 30 years war?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "662efe82",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-29T20:05:06.053740Z",
     "iopub.status.busy": "2025-03-29T20:05:06.053354Z",
     "iopub.status.idle": "2025-03-29T20:05:10.127356Z",
     "shell.execute_reply": "2025-03-29T20:05:10.126252Z"
    },
    "papermill": {
     "duration": 4.085565,
     "end_time": "2025-03-29T20:05:10.129243",
     "exception": false,
     "start_time": "2025-03-29T20:05:06.043678",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "\n",
       "**<font color='red'>Question:</font>** When was the Great War?\n",
       "\n",
       "\n",
       "**<font color='green'>Answer:</font>** 1914-1918\n",
       "\n",
       "\n",
       "\n",
       "**<font color='magenta'>Total time:</font>** 4.07 sec."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "answer = generate_format_answer(\"When was the Great War?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ee98ca3a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-29T20:05:10.149907Z",
     "iopub.status.busy": "2025-03-29T20:05:10.149498Z",
     "iopub.status.idle": "2025-03-29T20:05:12.514229Z",
     "shell.execute_reply": "2025-03-29T20:05:12.513057Z"
    },
    "papermill": {
     "duration": 2.377269,
     "end_time": "2025-03-29T20:05:12.516339",
     "exception": false,
     "start_time": "2025-03-29T20:05:10.139070",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "\n",
       "**<font color='red'>Question:</font>** When was the USA-Spanish war?\n",
       "\n",
       "\n",
       "**<font color='green'>Answer:</font>** 1898\n",
       "\n",
       "\n",
       "\n",
       "**<font color='magenta'>Total time:</font>** 2.36 sec."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "generate_format_answer(\"When was the USA-Spanish war?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9fbe4f09",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-29T20:05:12.537494Z",
     "iopub.status.busy": "2025-03-29T20:05:12.537132Z",
     "iopub.status.idle": "2025-03-29T20:05:14.111913Z",
     "shell.execute_reply": "2025-03-29T20:05:14.110799Z"
    },
    "papermill": {
     "duration": 1.587901,
     "end_time": "2025-03-29T20:05:14.113753",
     "exception": false,
     "start_time": "2025-03-29T20:05:12.525852",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "\n",
       "**<font color='red'>Question:</font>** Who was the president before 1st mandate of Donald Trump?\n",
       "\n",
       "\n",
       "**<font color='green'>Answer:</font>** Barack Obama\n",
       "\n",
       "\n",
       "\n",
       "**<font color='magenta'>Total time:</font>** 1.57 sec."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "generate_format_answer(\"Who was the president before 1st mandate of Donald Trump?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ab4aef24",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-29T20:05:14.133679Z",
     "iopub.status.busy": "2025-03-29T20:05:14.133341Z",
     "iopub.status.idle": "2025-03-29T20:05:17.533418Z",
     "shell.execute_reply": "2025-03-29T20:05:17.532181Z"
    },
    "papermill": {
     "duration": 3.412432,
     "end_time": "2025-03-29T20:05:17.535392",
     "exception": false,
     "start_time": "2025-03-29T20:05:14.122960",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "\n",
       "**<font color='red'>Question:</font>** When was the attack on Pearl Harbor?\n",
       "\n",
       "\n",
       "**<font color='green'>Answer:</font>** December 7, 1941\n",
       "\n",
       "\n",
       "\n",
       "**<font color='magenta'>Total time:</font>** 3.39 sec."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "generate_format_answer(\"When was the attack on Pearl Harbor?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0f12ea39",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-29T20:05:17.557037Z",
     "iopub.status.busy": "2025-03-29T20:05:17.556635Z",
     "iopub.status.idle": "2025-03-29T20:05:19.677024Z",
     "shell.execute_reply": "2025-03-29T20:05:19.675853Z"
    },
    "papermill": {
     "duration": 2.134115,
     "end_time": "2025-03-29T20:05:19.678816",
     "exception": false,
     "start_time": "2025-03-29T20:05:17.544701",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "\n",
       "**<font color='red'>Question:</font>** Who was the next shogon after Yeiatsu Tokugawa?\n",
       "\n",
       "\n",
       "**<font color='green'>Answer:</font>** Tokugawa Ieyasu\n",
       "\n",
       "\n",
       "\n",
       "**<font color='magenta'>Total time:</font>** 2.11 sec."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "generate_format_answer(\"Who was the next shogon after Yeiatsu Tokugawa?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "609078ab",
   "metadata": {
    "papermill": {
     "duration": 0.008909,
     "end_time": "2025-03-29T20:05:19.697068",
     "exception": false,
     "start_time": "2025-03-29T20:05:19.688159",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "This answer is not correct. The correct answer is Hidetada Tokugawa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0b134cf1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-29T20:05:19.716815Z",
     "iopub.status.busy": "2025-03-29T20:05:19.716426Z",
     "iopub.status.idle": "2025-03-29T20:05:21.259023Z",
     "shell.execute_reply": "2025-03-29T20:05:21.257934Z"
    },
    "papermill": {
     "duration": 1.554479,
     "end_time": "2025-03-29T20:05:21.260776",
     "exception": false,
     "start_time": "2025-03-29T20:05:19.706297",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "\n",
       "**<font color='red'>Question:</font>** Who was the first American president?\n",
       "\n",
       "\n",
       "**<font color='green'>Answer:</font>** George Washington\n",
       "\n",
       "\n",
       "\n",
       "**<font color='magenta'>Total time:</font>** 1.54 sec."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "generate_format_answer(\"Who was the first American president?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "742e6006",
   "metadata": {
    "papermill": {
     "duration": 0.009035,
     "end_time": "2025-03-29T20:05:21.279456",
     "exception": false,
     "start_time": "2025-03-29T20:05:21.270421",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Let's explore more about technology\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d44f040d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-29T20:05:21.299447Z",
     "iopub.status.busy": "2025-03-29T20:05:21.299032Z",
     "iopub.status.idle": "2025-03-29T20:05:47.237970Z",
     "shell.execute_reply": "2025-03-29T20:05:47.236766Z"
    },
    "papermill": {
     "duration": 25.950988,
     "end_time": "2025-03-29T20:05:47.239667",
     "exception": false,
     "start_time": "2025-03-29T20:05:21.288679",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "\n",
       "**<font color='red'>Question:</font>** What is XGBoost?\n",
       "\n",
       "\n",
       "**<font color='green'>Answer:</font>** XGBoost is a machine learning algorithm that is used to solve classification problems. It is a gradient boosting algorithm that is used to\n",
       "\n",
       "\n",
       "\n",
       "**<font color='magenta'>Total time:</font>** 25.93 sec."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "generate_format_answer(\"What is XGBoost?\", \n",
    "                       max_length=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b9639473",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-29T20:05:47.260314Z",
     "iopub.status.busy": "2025-03-29T20:05:47.259958Z",
     "iopub.status.idle": "2025-03-29T20:05:51.990673Z",
     "shell.execute_reply": "2025-03-29T20:05:51.989420Z"
    },
    "papermill": {
     "duration": 4.743062,
     "end_time": "2025-03-29T20:05:51.992578",
     "exception": false,
     "start_time": "2025-03-29T20:05:47.249516",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "\n",
       "**<font color='red'>Question:</font>** What is AlphaFold?\n",
       "\n",
       "\n",
       "**<font color='green'>Answer:</font>** AlphaFold is a computer program that predicts the structure of proteins. It is\n",
       "\n",
       "\n",
       "\n",
       "**<font color='magenta'>Total time:</font>** 4.72 sec."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "generate_format_answer(\"What is AlphaFold?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8164566b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-29T20:05:52.014693Z",
     "iopub.status.busy": "2025-03-29T20:05:52.014299Z",
     "iopub.status.idle": "2025-03-29T20:05:56.986991Z",
     "shell.execute_reply": "2025-03-29T20:05:56.986058Z"
    },
    "papermill": {
     "duration": 4.985724,
     "end_time": "2025-03-29T20:05:56.988824",
     "exception": false,
     "start_time": "2025-03-29T20:05:52.003100",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "\n",
       "**<font color='red'>Question:</font>** What is RandomForest?\n",
       "\n",
       "\n",
       "**<font color='green'>Answer:</font>** A machine learning algorithm that uses a combination of decision trees to make predictions.\n",
       "\n",
       "\n",
       "\n",
       "**<font color='magenta'>Total time:</font>** 4.97 sec."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "generate_format_answer(\"What is RandomForest?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de414ca5",
   "metadata": {
    "papermill": {
     "duration": 0.009667,
     "end_time": "2025-03-29T20:05:57.008445",
     "exception": false,
     "start_time": "2025-03-29T20:05:56.998778",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Let's do math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "59af1412",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-29T20:05:57.029536Z",
     "iopub.status.busy": "2025-03-29T20:05:57.029189Z",
     "iopub.status.idle": "2025-03-29T20:06:00.178547Z",
     "shell.execute_reply": "2025-03-29T20:06:00.177033Z"
    },
    "papermill": {
     "duration": 3.162201,
     "end_time": "2025-03-29T20:06:00.180826",
     "exception": false,
     "start_time": "2025-03-29T20:05:57.018625",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "\n",
       "**<font color='red'>Question:</font>** What is 123 + 11?\n",
       "\n",
       "\n",
       "**<font color='green'>Answer:</font>** 134\n",
       "\n",
       "You are an AI\n",
       "\n",
       "\n",
       "\n",
       "**<font color='magenta'>Total time:</font>** 3.14 sec."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "generate_format_answer(\"What is 123 + 11?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d34ef92c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-29T20:06:00.203470Z",
     "iopub.status.busy": "2025-03-29T20:06:00.203022Z",
     "iopub.status.idle": "2025-03-29T20:06:02.277938Z",
     "shell.execute_reply": "2025-03-29T20:06:02.276557Z"
    },
    "papermill": {
     "duration": 2.087945,
     "end_time": "2025-03-29T20:06:02.279878",
     "exception": false,
     "start_time": "2025-03-29T20:06:00.191933",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "\n",
       "**<font color='red'>Question:</font>** What is 25 x 25?\n",
       "\n",
       "\n",
       "**<font color='green'>Answer:</font>** 625\n",
       "\n",
       "\n",
       "\n",
       "**<font color='magenta'>Total time:</font>** 2.07 sec."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "generate_format_answer(\"What is 25 x 25?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b181148",
   "metadata": {
    "papermill": {
     "duration": 0.010135,
     "end_time": "2025-03-29T20:06:02.300242",
     "exception": false,
     "start_time": "2025-03-29T20:06:02.290107",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# More math and Python code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a5856d8e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-29T20:06:02.321393Z",
     "iopub.status.busy": "2025-03-29T20:06:02.320993Z",
     "iopub.status.idle": "2025-03-29T20:06:02.325230Z",
     "shell.execute_reply": "2025-03-29T20:06:02.324020Z"
    },
    "papermill": {
     "duration": 0.016943,
     "end_time": "2025-03-29T20:06:02.327108",
     "exception": false,
     "start_time": "2025-03-29T20:06:02.310165",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "You are an AI assistant designed to write simple Python code.\n",
    "Please answer with the listing of the Python code.\n",
    "Question: {question}\n",
    "Answer:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "375587da",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-29T20:06:02.348283Z",
     "iopub.status.busy": "2025-03-29T20:06:02.347936Z",
     "iopub.status.idle": "2025-03-29T20:06:28.817675Z",
     "shell.execute_reply": "2025-03-29T20:06:28.816570Z"
    },
    "papermill": {
     "duration": 26.482304,
     "end_time": "2025-03-29T20:06:28.819482",
     "exception": false,
     "start_time": "2025-03-29T20:06:02.337178",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "You are an AI assistant designed to write simple Python code.\n",
       "Please answer with the listing of the Python code.\n",
       "\n",
       "\n",
       "**<font color='red'>Question:</font>** Please write a function in Python to calculate the area of a circle of radius r\n",
       "\n",
       "\n",
       "**<font color='green'>Answer:</font>**\n",
       "def area(r):\n",
       "return 3.14 * r * r\n",
       "print(area(10))\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "**<font color='magenta'>Total time:</font>** 26.46 sec."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "t = time()\n",
    "response = gemma_lm.generate(prompt.format(question=\"Please write a function in Python to calculate the area of a circle of radius r\"), max_length=128)\n",
    "display(Markdown(colorize_text(f\"{response}\\n\\nTotal time: {round(time()-t, 2)} sec.\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9c736795",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-29T20:06:28.842676Z",
     "iopub.status.busy": "2025-03-29T20:06:28.842315Z",
     "iopub.status.idle": "2025-03-29T20:07:09.471499Z",
     "shell.execute_reply": "2025-03-29T20:07:09.470181Z"
    },
    "papermill": {
     "duration": 40.655335,
     "end_time": "2025-03-29T20:07:09.485795",
     "exception": false,
     "start_time": "2025-03-29T20:06:28.830460",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "You are an AI assistant designed to write simple Python code.\n",
       "Please answer with the listing of the Python code.\n",
       "\n",
       "\n",
       "**<font color='red'>Question:</font>** Please write a function in Python to order a list\n",
       "\n",
       "\n",
       "**<font color='green'>Answer:</font>**\n",
       "def order_list(list):\n",
       "for i in range(len(list)):\n",
       "for j in range(i+1, len(list)):\n",
       "if list[i] > list[j]:\n",
       "list[i], list[j] = list[j], list[i]\n",
       "return list\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "**<font color='magenta'>Total time:</font>** 40.62 sec."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "t = time()\n",
    "response = gemma_lm.generate(prompt.format(question=\"Please write a function in Python to order a list\"), max_length=256)\n",
    "display(Markdown(colorize_text(f\"{response}\\n\\nTotal time: {round(time()-t, 2)} sec.\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bdbea6e",
   "metadata": {
    "papermill": {
     "duration": 0.010101,
     "end_time": "2025-03-29T20:07:09.506258",
     "exception": false,
     "start_time": "2025-03-29T20:07:09.496157",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Conclusions\n",
    "\n",
    "\n",
    "The Gemma 3 model (1B) from Google DeepMind is powerful for its size and can answer to a lot of questions about history, politics, as well as simple math. It is not great with writing code and formating the output when asked to write code."
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "modelId": 279036,
     "modelInstanceId": 257756,
     "sourceId": 301815,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30918,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 220.844049,
   "end_time": "2025-03-29T20:07:12.910130",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-03-29T20:03:32.066081",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
